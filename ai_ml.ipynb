{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk2tiyE087a5Yhej01f8A1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susik17/collage_lab_expirements/blob/main/ai_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn model"
      ],
      "metadata": {
        "id": "hEshXuolEpsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Szdoyt-EjrI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "# One-hot encode the target labels\n",
        "enc = OneHotEncoder()\n",
        "y = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "random_state=42)\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "Dense(10, activation=’relu’, input_shape=(X_train.shape[1],)),\n",
        "Dense(3, activation=’softmax’)\n",
        "])\n",
        "# Compile the model\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=’adam’, loss=’categorical_crossentropy’,\n",
        "metrics=[‘accuracy’])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=5, validation_split=0.1)\n",
        "# Evaluate the model on the testing set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f”test Loss: {loss}, Test Accuracy: {accuracy}”)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "deeep learning"
      ],
      "metadata": {
        "id": "KHd4yq-3FGMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "77\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "random_state=42)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Build the deep learning neural network model\n",
        "model = Sequential([\n",
        "Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "Dense(16, activation='relu'),Dense(3, activation='softmax') # Three output neurons for three iris species\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "s7MGAlOjFIln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "exc max algorithm"
      ],
      "metadata": {
        "id": "fPT0hm9UFbdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.mixture import GaussianMixture\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "# Initialize the Gaussian Mixture Model with 3 components\n",
        "gmm = GaussianMixture(n_components=3)\n",
        "# Fit the model using the EM algorithM\n",
        "gmm.fit(X)\n",
        "# Get the cluster assignments for each data point\n",
        "labels = gmm.predict(X)\n",
        "# Print the cluster means and covariances\n",
        "print(\"Cluster Means:\")\n",
        "print(gmm.means)\n",
        "print(\"\\nCluster Covariances:\")\n",
        "print(gmm.covariances)"
      ],
      "metadata": {
        "id": "45e5ulynFhUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k means"
      ],
      "metadata": {
        "id": "cdr-LA-HFqW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "#load data from csv file\n",
        "df = pd.read_csv(“C:/Users/Desktop/AzureDiabDataset.csv”)\n",
        "# Visualize the original data\n",
        "plt.scatter(df['AGE'], df['BP'], s=50,color='blue', label='Original Data')\n",
        "#Apply k-means clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(df[['AGE', 'BP']])\n",
        "#Visualize the clustered data\n",
        "\n",
        "53\n",
        "\n",
        "plt.scatter(df['AGE'], df['BP'], c=df['Cluster'], cmap='viridis', s=50, alpha=0.8,\n",
        "edgecolors='w', label='Clustered Data')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red',\n",
        "marker='X', s=200, label='Centroids')\n",
        "#Add labels and legends\n",
        "plt.title('K-Means Clustering')\n",
        "plt.xlabel('AGE')\n",
        "plt.ylabel('BP')\n",
        "plt.legend()\n",
        "#show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GVA76c-pFsP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ensemble"
      ],
      "metadata": {
        "id": "OzAhHE8pFyod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,\n",
        "BaggingClassifier, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Bagging with Random Forest\n",
        "bagging_clf = BaggingClassifier(RandomForestClassifier(n_estimators=10,\n",
        "random_state=42), n_estimators=5, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X)\n",
        "bagging_accuracy = accuracy_score(y, bagging_pred)\n",
        "print(\"Bagging Accuracy:\", bagging_accuracy)\n",
        "print(\"Bagging Predictions:\", bagging_pred)\n",
        "# Boosting with AdaBoost\n",
        "boosting_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "boosting_clf.fit(X_train, y_train)\n",
        "boosting_pred = boosting_clf.predict(X)\n",
        "boosting_accuracy = accuracy_score(y, boosting_pred)\n",
        "print(\"Boosting Accuracy:\", boosting_accuracy)\n",
        "print(\"Boosting Predictions:\", boosting_pred)\n",
        "# Stacking with Random Forest, AdaBoost, and Logistic Regression\n",
        "base_models = [\n",
        "('random_forest', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "('adaboost', AdaBoostClassifier(n_estimators=10, random_state=42))\n",
        "]\n",
        "meta_learner = LogisticRegression()\n",
        "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_learner)\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "stacking_pred = stacking_clf.predict(X)\n",
        "stacking_accuracy = accuracy_score(y, stacking_pred)\n",
        "print(\"Stacking Accuracy:\", stacking_accuracy)\n",
        "print(\"Stacking Predictions:\", stacking_pred)"
      ],
      "metadata": {
        "id": "bhXqoNP0F1oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "naive bayes"
      ],
      "metadata": {
        "id": "acB0ID_rF_JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(\"note.csv\") # Update \"your_file.csv\" with the actual file path\n",
        "# Drop rows with NaN values\n",
        "data.dropna(subset=['text', 'spam'], inplace=True)\n",
        "# Extract the \"spam\" column as labels and the \"text\" column as training sentences\n",
        "train_sentences = data['text'].tolist()\n",
        "train_labels = data['spam'].tolist()\n",
        "# Vectorize the training data\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_sentences)\n",
        "# Train the classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, train_labels)\n",
        "# Get user input\n",
        "print(\"Enter a Sentence to check whether the text is spam or not\")\n",
        "user_input = input(\"Enter a sentence: \")\n",
        "# Vectorize the user input\n",
        "X_user = vectorizer.transform([user_input])\n",
        "# Predict label\n",
        "prediction = nb_classifier.predict(X_user)[0]\n",
        "# Print prediction\n",
        "spam_or_not = \"spam\" if prediction == 1 else \"not spam\"\n",
        "print(f\"Predicted label for '{user_input}': {spam_or_not}\")"
      ],
      "metadata": {
        "id": "IAY5RuACGBzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bayesian network"
      ],
      "metadata": {
        "id": "rLpJCBfBGSJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "from pgmpy.inference import VariableElimination\n",
        "# Load the datasets\n",
        "data1 = pd.read_excel('drug.xlsx')\n",
        "data2 = pd.read_excel('age.xlsx')\n",
        "# Binning the Age\n",
        "data1['Age_binned'] = pd.cut(data1['Age'], bins=[0, 30, 60, 100], labels=['Young',\n",
        "'Middle-aged', 'Old'])\n",
        "data2['Age_binned'] = pd.cut(data2['Age'], bins=[0, 30, 60, 100], labels=['Young',\n",
        "'Middle-aged', 'Old'])\n",
        "# Converting categorical variables to numerical codes\n",
        "data1['Sex'] = data1['Sex'].map({'F': 0, 'M': 1})\n",
        "data2['gender'] = data2['gender'].map({'F': 0, 'M': 1})\n",
        "# Renaming columns to have consistent naming\n",
        "data2.rename(columns={'gender': 'Sex'}, inplace=True)\n",
        "# Merging datasets on common columns\n",
        "combined_data = pd.merge(data1, data2, on=['Age_binned', 'Sex'])\n",
        "# Inspecting the merged data to ensure it contains the expected columns\n",
        "print(\"Combined data columns:\", combined_data.columns)\n",
        "# Use one of the Cholesterol columns and rename it for consistency\n",
        "combined_data['Cholesterol'] = combined_data['Cholesterol_x']\n",
        "# Simplifying data\n",
        "combined_data = combined_data[['Age_binned', 'Sex', 'Drug', 'Cholesterol']]\n",
        "# Define the Bayesian Network structure\n",
        "model = BayesianModel([('Age_binned', 'Drug'), ('Sex', 'Drug'), ('Age_binned',\n",
        "'Cholesterol'), ('Sex', 'Cholesterol'), ('Drug', 'Cholesterol')])\n",
        "# Fit the model with Maximum Likelihood Estimation\n",
        "model.fit(combined_data, estimator=MaximumLikelihoodEstimator)\n",
        "# Perform inference\n",
        "infer = VariableElimination(model)\n",
        "# Example inference\n",
        "query = infer.query(variables=['Drug', 'Cholesterol'], evidence={'Age_binned': 'Young',\n",
        "'Sex': 0})\n",
        "print(\"P(Drug, Cholesterol | Age=Young, Sex=0):\")\n",
        "print(query)"
      ],
      "metadata": {
        "id": "n11khmuCGU0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}